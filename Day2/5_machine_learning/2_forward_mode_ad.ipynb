{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What AD is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic differentiation:**\n",
    "$$ \\frac{d}{dx}x^n = n x^{n-1}. $$\n",
    "\n",
    "**Numerical differentiation:**\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x+h) - f(x)}{\\Delta h} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The example and the figures below are taken from [this book](https://mitpress.ublish.com/book/algorithms-for-optimization).)\n",
    "\n",
    "Key to AD is the application of the chain rule\n",
    "$$\\dfrac{d}{dx} f(g(x) = \\dfrac{df}{dg} \\dfrac{dg}{dx}$$\n",
    "\n",
    "Consider the function $f(a,b) = \\ln(ab + \\max(a,2))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) = log(a*b + sin(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_derivative(a,b) = 1/(a*b + sin(a)) * (b + cos(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3.1\n",
    "b = 2.4\n",
    "f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the function into the elementary steps, it corresponds to the following *computational graph*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph(a,b)\n",
    "    c1 = a*b\n",
    "    c2 = sin(a)\n",
    "    c3 = c1 + c2\n",
    "    c4 = log(c3)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(a,b) == f_graph(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $\\frac{\\partial f}{\\partial a}$ we have to apply the chain rule multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{df}{dx} = \\dfrac{df}{dc_4} \\dfrac{dc_4}{dx} = \\dfrac{df}{dc_4} \\left( \\dfrac{dc_4}{dc_3} \\dfrac{dc_3}{dx}  \\right) = \\dfrac{df}{dc_4} \\left( \\dfrac{dc_4}{dc_3} \\left( \\dfrac{dc_3}{dc_2} \\dfrac{dc_2}{dx} + \\dfrac{dc_3}{dc_1} \\dfrac{dc_1}{dx}\\right)  \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode AD will automatically differentiate the function using a forward pass through the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D for \"dual number\", invented by Clifford in 1873.\n",
    "struct D <: Number\n",
    "    x::Float64 # value\n",
    "    ϵ::Float64 # derivative\n",
    "end\n",
    "\n",
    "import Base: +, *, /, -, sin, log, convert, promote_rule\n",
    "\n",
    "a::D + b::D = D(a.x + b.x, a.ϵ + b.ϵ) # sum rule\n",
    "a::D - b::D = D(a.x - b.x, a.ϵ - b.ϵ)\n",
    "a::D * b::D = D(a.x * b.x, a.x * b.ϵ + a.ϵ * b.x) # product rule\n",
    "a::D / b::D = D(a.x / b.x, (b.x * a.ϵ - a.x * b.ϵ)/b.x^2) # quotient rule\n",
    "sin(a::D) = D(sin(a.x), cos(a.x) * a.ϵ)\n",
    "log(a::D) = D(log(a.x), 1/a.x * a.ϵ)\n",
    "\n",
    "Base.convert(::Type{D}, x::Real) = D(x, zero(x))\n",
    "Base.promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(D(a,1), b).ϵ ≈ f_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does this work?!**\n",
    "\n",
    "Note that we can take derivatives of every node in the graph (elementary operation) to obtain the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_graph_derivative(a,b)\n",
    "    c1 = a*b\n",
    "    c1_ϵ = b\n",
    "    \n",
    "    c2 = sin(a)\n",
    "    c2_ϵ = cos(a)\n",
    "    \n",
    "    c3 = c1 + c2\n",
    "    c3_ϵ = c1_ϵ + c2_ϵ\n",
    "    \n",
    "    c4 = log(c3)\n",
    "    c4_ϵ = 1/c3 * c3_ϵ\n",
    "    \n",
    "    c4, c4_ϵ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_graph_derivative(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The trick of our dual number based forward mode AD is to let the computer do this rewrite for you!**\n",
    "\n",
    "Importantly, the compiler sees the entire \"rewritten\" code and can therefore apply all kinds of optimizations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f_graph_derivative(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none f(D(a,1), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for our small forward AD\n",
    "derivative(f::Function, x::Number) = f(D(x, one(x))).ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->f(x,b), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->f(x,b), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->3*x^2+4x+5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x->sin(x)*log(x), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x) = derivative(a->f(a,b),x) # partial derivative wrt a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(1.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking derivatives of code: Babylonian sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat $t \\leftarrow (t + x/2)/2$ until $t$ converges to $\\sqrt{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function Babylonian(x; N = 10)\n",
    "    t = (1+x)/2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t)/2\n",
    "    end\n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(2), √2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "xs = 0:0.01:49\n",
    "\n",
    "p = plot(title = \"Those Babylonians really knew what they were doing\")\n",
    "for i in 1:5\n",
    "    plot!(p, xs, [Babylonian(x; N=i) for x in xs], label=\"Iteration $i\")\n",
    "end\n",
    "\n",
    "plot!(p, xs, sqrt.(xs), label=\"sqrt\", color=:black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and now the derivative, automagically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same babylonian algorithm with no rewrite at all computes properly the derivative as the check shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "√5, 0.5 / √5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It just works and is efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none Babylonian(D(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically (because we can)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is mathematically equivalent, though not what the computation is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")\n",
    "\n",
    "display(\"Iterations as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(Babylonian(x; N=k)))\n",
    "end\n",
    "\n",
    "display(\"Derivatives as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(diff(simplify(Babylonian(x; N=k)), x)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ForwardDiff.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood how forward AD works, we can use the more feature complete package [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@edit ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: [DiffRules.jl](https://github.com/JuliaDiff/DiffRules.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode:\n",
    "$\\dfrac{df}{dx} = \\dfrac{df}{dc_4} \\dfrac{dc_4}{dx} = \\dfrac{df}{dc_4} \\left( \\dfrac{dc_4}{dc_3} \\dfrac{dc_3}{dx}  \\right) = \\dfrac{df}{dc_4} \\left( \\dfrac{dc_4}{dc_3} \\left( \\dfrac{dc_3}{dc_2} \\dfrac{dc_2}{dx} + \\dfrac{dc_3}{dc_1} \\dfrac{dc_1}{dx}\\right)  \\right)$\n",
    "\n",
    "Reverse mode:\n",
    "$\\dfrac{df}{dx} = \\dfrac{df}{dc_4} \\dfrac{dc_4}{dx} = \\left( \\dfrac{df}{dc_3}\\dfrac{dc_3}{dc_4}   \\right) \\dfrac{dc_4}{dx} = \\left( \\left( \\dfrac{df}{dc_2} \\dfrac{dc_2}{dc_3} + \\dfrac{df}{dc_1} \\dfrac{dc_1}{dc_3} \\right) \\dfrac{dc_3}{dc_4} \\right) \\dfrac{dc_4}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward mode AD requires $n$ passes in order to compute an $n$-dimensional\n",
    "gradient.\n",
    "\n",
    "Reverse mode AD requires only a single run in order to compute a complete gradient but requires two passes through the graph: a forward pass during which necessary intermediate values are computed and a backward pass which computes the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rule of thumb:*\n",
    "\n",
    "Forward mode is good for $\\mathbb{R} \\rightarrow \\mathbb{R}^n$ while reverse mode is good for $\\mathbb{R}^n \\rightarrow \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some nice reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog posts:\n",
    "\n",
    "* ML in Julia: https://julialang.org/blog/2018/12/ml-language-compiler\n",
    "\n",
    "* Nice example: https://fluxml.ai/2019/03/05/dp-vs-rl.html\n",
    "\n",
    "* Nice interactive examples: https://fluxml.ai/experiments/\n",
    "\n",
    "* Why Julia for ML? https://julialang.org/blog/2017/12/ml&pl\n",
    "\n",
    "* Neural networks with differential equation layers: https://julialang.org/blog/2019/01/fluxdiffeq\n",
    "\n",
    "* Implement Your Own Automatic Differentiation with Julia in ONE day : http://blog.rogerluo.me/2018/10/23/write-an-ad-in-one-day/\n",
    "\n",
    "* Implement Your Own Source To Source AD in ONE day!: http://blog.rogerluo.me/2019/07/27/yassad/\n",
    "\n",
    "Repositories:\n",
    "\n",
    "* AD flavors, like forward and reverse mode AD: https://github.com/MikeInnes/diff-zoo (Mike is one of the smartest Julia ML heads)\n",
    "\n",
    "Talks:\n",
    "\n",
    "* AD is a compiler problem: https://juliacomputing.com/assets/pdf/CGO_C4ML_talk.pdf"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
